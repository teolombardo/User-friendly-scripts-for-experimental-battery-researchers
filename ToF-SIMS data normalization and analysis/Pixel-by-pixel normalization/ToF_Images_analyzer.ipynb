{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e93e460d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results path = D:\\Teo\\ProGral\\Prali\\Data\\ToF-SIMS\\Analyses\\Pre_lithiation_project\\Pixel_normalization\\Non cycled\n",
      "Important ion fragment = ['LiOH-', 'LiO_2-', 'LiCO_3-', 'LiCO_2-', 'LiF_2-', 'Li_2F_3-', 'Li_4F_5-', 'Li_5F_6-', 'Li_3F_4-', 'LiPO_3-', 'LiPO_4-', 'LiPO_2-', 'LiPO-']\n",
      "Normalization peak = C_2-\n"
     ]
    }
   ],
   "source": [
    "#Author: Teo Lombardo\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import plotly.express as px\n",
    "from skimage import io, img_as_float, img_as_ubyte\n",
    "from skimage.filters import gaussian\n",
    "from skimage.restoration import denoise_nl_means, estimate_sigma\n",
    "from skimage.exposure import equalize_adapthist\n",
    "import pickle\n",
    "\n",
    "# Reading the input file\n",
    "Inputs = pd.read_excel (r'Inputs_ToF_Images.xlsx')\n",
    "\n",
    "path_data = (Inputs[\"Parameters' value\"][1])\n",
    "print(\"Results path =\", path_data)\n",
    "\n",
    "splitted_peaks = Inputs[\"Parameters' value\"][2].split(\",\")\n",
    "for i in range(len(splitted_peaks)):\n",
    "    important_peaks = [x.replace(\" \", \"\") for x in splitted_peaks]\n",
    "print(\"Important ion fragment =\", important_peaks)\n",
    "\n",
    "norm_peak = str(Inputs[\"Parameters' value\"][3])\n",
    "if norm_peak == \"nan\":\n",
    "    norm_peak = \"\"\n",
    "print(\"Normalization peak =\", norm_peak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "641780e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check previous\n",
    "Samples_dir = [x[0] for x in os.walk(path_data)]\n",
    "test_previous = path_data + \"\\\\Box_plots_results_img\"\n",
    "previous = 0\n",
    "if test_previous in Samples_dir:\n",
    "    previous = 1\n",
    "\n",
    "#previous = 0\n",
    "print(previous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7bb975d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\Teo\\\\ProGral\\\\Prali\\\\Data\\\\ToF-SIMS\\\\Analyses\\\\Pre_lithiation_project\\\\Pixel_normalization\\\\Non cycled',\n",
       " 'D:\\\\Teo\\\\ProGral\\\\Prali\\\\Data\\\\ToF-SIMS\\\\Analyses\\\\Pre_lithiation_project\\\\Pixel_normalization\\\\Non cycled\\\\00_Ref',\n",
       " \"D:\\\\Teo\\\\ProGral\\\\Prali\\\\Data\\\\ToF-SIMS\\\\Analyses\\\\Pre_lithiation_project\\\\Pixel_normalization\\\\Non cycled\\\\0_5' - 20\",\n",
       " \"D:\\\\Teo\\\\ProGral\\\\Prali\\\\Data\\\\ToF-SIMS\\\\Analyses\\\\Pre_lithiation_project\\\\Pixel_normalization\\\\Non cycled\\\\0_5' - 40\",\n",
       " \"D:\\\\Teo\\\\ProGral\\\\Prali\\\\Data\\\\ToF-SIMS\\\\Analyses\\\\Pre_lithiation_project\\\\Pixel_normalization\\\\Non cycled\\\\0_5' - 60\",\n",
       " 'D:\\\\Teo\\\\ProGral\\\\Prali\\\\Data\\\\ToF-SIMS\\\\Analyses\\\\Pre_lithiation_project\\\\Pixel_normalization\\\\Non cycled\\\\1h - 20',\n",
       " 'D:\\\\Teo\\\\ProGral\\\\Prali\\\\Data\\\\ToF-SIMS\\\\Analyses\\\\Pre_lithiation_project\\\\Pixel_normalization\\\\Non cycled\\\\1h - 40',\n",
       " 'D:\\\\Teo\\\\ProGral\\\\Prali\\\\Data\\\\ToF-SIMS\\\\Analyses\\\\Pre_lithiation_project\\\\Pixel_normalization\\\\Non cycled\\\\1h - 60',\n",
       " 'D:\\\\Teo\\\\ProGral\\\\Prali\\\\Data\\\\ToF-SIMS\\\\Analyses\\\\Pre_lithiation_project\\\\Pixel_normalization\\\\Non cycled\\\\24h - 20',\n",
       " 'D:\\\\Teo\\\\ProGral\\\\Prali\\\\Data\\\\ToF-SIMS\\\\Analyses\\\\Pre_lithiation_project\\\\Pixel_normalization\\\\Non cycled\\\\24h - 40',\n",
       " 'D:\\\\Teo\\\\ProGral\\\\Prali\\\\Data\\\\ToF-SIMS\\\\Analyses\\\\Pre_lithiation_project\\\\Pixel_normalization\\\\Non cycled\\\\24h -60 - white',\n",
       " \"D:\\\\Teo\\\\ProGral\\\\Prali\\\\Data\\\\ToF-SIMS\\\\Analyses\\\\Pre_lithiation_project\\\\Pixel_normalization\\\\Non cycled\\\\Air - 0_5'\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Samples_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f03d5d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_all = {}\n",
    "normalization_dict = {}\n",
    "dict_sums = {}\n",
    "for directory in Samples_dir[1:]:\n",
    "    if \"Box_plots_results_img\" not in directory:\n",
    "        sample_name = directory.split(\"\\\\\")[-1]\n",
    "        #print(sample_name)\n",
    "        dict_all[sample_name] = {}\n",
    "        normalization_dict[sample_name] = {}\n",
    "        measurements = os.listdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f1ce52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing and filling dictionaries (first run)\n",
    "\n",
    "if previous == 0:\n",
    "    dict_all = {}\n",
    "    normalization_dict = {}\n",
    "    dict_sums = {}\n",
    "    for directory in Samples_dir[1:]:\n",
    "        if \"Box_plots_results_img\" not in directory:\n",
    "            sample_name = directory.split(\"\\\\\")[-1]\n",
    "            #print(sample_name)\n",
    "            dict_all[sample_name] = {}\n",
    "            normalization_dict[sample_name] = {}\n",
    "            measurements = os.listdir(directory)\n",
    "\n",
    "            for img_n in measurements:\n",
    "                measure_name = img_n.split(\" (\")[0]\n",
    "                peak_name = img_n.split(\") \")[1].split(\"- \")[1].split(\".txt\")[0]\n",
    "                #print(measure_name, peak_name)\n",
    "                dict_all[sample_name][measure_name] = {}\n",
    "                normalization_dict[sample_name][measure_name] = {}\n",
    "            for img_n in measurements:\n",
    "                measure_name = img_n.split(\" (\")[0]\n",
    "                peak_name = img_n.split(\") \")[1].split(\"- \")[1].split(\".txt\")[0]\n",
    "                #print(measure_name, peak_name)\n",
    "                dict_all[sample_name][measure_name][peak_name] = {}\n",
    "                peak_dict = dict_all[sample_name][measure_name][peak_name]\n",
    "                if peak_name == norm_peak:\n",
    "                    normalization_dict[sample_name][measure_name][peak_name] = {}\n",
    "                    Norm_to_fill = normalization_dict[sample_name][measure_name][peak_name]\n",
    "                    \n",
    "                path_img = directory + \"\\\\\" + img_n\n",
    "                if peak_name != \"total\":\n",
    "                    dict_sums[peak_name] = {}\n",
    "                    dict_sums[peak_name][\"Sample\"] = []\n",
    "                    dict_sums[peak_name][\"Intensity\"] = []\n",
    "                    dict_sums[peak_name][\"Normalized intensity\"] = []\n",
    "                    if len(norm_peak)>0:\n",
    "                        dict_sums[peak_name][\"Custom normalized intensity\"] = []\n",
    "\n",
    "                with open(path_img, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                    f.close()\n",
    "\n",
    "                count_line = 0\n",
    "                for line in lines:\n",
    "                    splitted_line = line.split(\" \")\n",
    "                    #print(splitted_line)\n",
    "                    if splitted_line[0] != \"#\":\n",
    "                        header = count_line+1\n",
    "                        break\n",
    "                    count_line+=1\n",
    "\n",
    "                max_pixel = 0\n",
    "                max_I = 0\n",
    "                for line in lines[header:]:\n",
    "                    splitted_line = line.split(\" \")\n",
    "                    pixel = float(splitted_line[0])\n",
    "                    if pixel == 0:\n",
    "                        max_pixel += 1\n",
    "                    I = float(splitted_line[2])\n",
    "                    if I > max_I:\n",
    "                        max_I = I\n",
    "\n",
    "                peak_dict[\"I\"] = [[] for x in range(max_pixel)]\n",
    "                peak_dict[\"In\"] = [[] for x in range(max_pixel)]\n",
    "                if peak_name == norm_peak:\n",
    "                    Norm_to_fill[\"I\"] = [[] for x in range(max_pixel)]\n",
    "                for line in lines[header:]:\n",
    "                    splitted_line = line.split(\" \")\n",
    "                    X = int(splitted_line[0]) \n",
    "                    Y = int(splitted_line[1]) \n",
    "                    I = float(splitted_line[2])\n",
    "                    if max_I >0:\n",
    "                        In = I/max_I\n",
    "                    else: # To be taken in mind, it is weird that I need this\n",
    "                        In = 0\n",
    "                    peak_dict[\"I\"][Y].append(I) \n",
    "                    peak_dict[\"In\"][Y].append(In) \n",
    "                    if peak_name == norm_peak:\n",
    "                        Norm_to_fill[\"I\"][Y].append(I)\n",
    "                peak_dict[\"I\"] = np.array(peak_dict[\"I\"])\n",
    "                peak_dict[\"In\"] = np.array(peak_dict[\"In\"])\n",
    "                if peak_name == norm_peak:\n",
    "                    yi = 0\n",
    "                    for y in Norm_to_fill[\"I\"]:\n",
    "                        xi = 0\n",
    "                        for x in y:\n",
    "                            if Norm_to_fill[\"I\"][yi][xi] == 0:\n",
    "                                Norm_to_fill[\"I\"][yi][xi] = 0.5\n",
    "                            xi+=1\n",
    "                        yi+=1\n",
    "                    Norm_to_fill[\"I\"] = np.array(Norm_to_fill[\"I\"])\n",
    "                \n",
    "    #pkl_filename = \"dict_all_images.pkl\"\n",
    "    #with open(pkl_filename, 'wb') as file:\n",
    "    #    pickle.dump(dict_all, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dd5011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing and filling normalization dictionary (not first run)\n",
    "if previous == 1 and len(norm_peak)>0:\n",
    "    print(\"Restarting\")\n",
    "    pkl_filename = path_data + \"\\\\\" + \"dict_all_images.pkl\"\n",
    "    with open(pkl_filename, 'rb') as file:\n",
    "        dict_all = pickle.load(file)\n",
    "        \n",
    "    normalization_dict = {}\n",
    "    dict_sums = {}\n",
    "    for directory in Samples_dir[1:]:\n",
    "        if \"Box_plots_results_img\" not in directory:\n",
    "            sample_name = directory.split(\"\\\\\")[-1]\n",
    "            normalization_dict[sample_name] = {}\n",
    "            measurements = os.listdir(directory)\n",
    "\n",
    "            for img_n in measurements:\n",
    "                measure_name = img_n.split(\" (\")[0]\n",
    "                peak_name = img_n.split(\") \")[1].split(\"- \")[1].split(\".txt\")[0]\n",
    "                normalization_dict[sample_name][measure_name] = {}\n",
    "            for img_n in measurements:\n",
    "                measure_name = img_n.split(\" (\")[0]\n",
    "                peak_name = img_n.split(\") \")[1].split(\"- \")[1].split(\".txt\")[0]\n",
    "                if peak_name == norm_peak:\n",
    "                    normalization_dict[sample_name][measure_name][peak_name] = {}\n",
    "                    Norm_to_fill = normalization_dict[sample_name][measure_name][peak_name]\n",
    "                    \n",
    "                path_img = directory + \"\\\\\" + img_n\n",
    "                if peak_name != \"total\":\n",
    "                    dict_sums[peak_name] = {}\n",
    "                    dict_sums[peak_name][\"Sample\"] = []\n",
    "                    dict_sums[peak_name][\"Intensity\"] = []\n",
    "                    dict_sums[peak_name][\"Normalized intensity\"] = []\n",
    "                    if len(norm_peak)>0:\n",
    "                        dict_sums[peak_name][\"Custom normalized intensity\"] = []\n",
    "\n",
    "                if peak_name == norm_peak:\n",
    "                    with open(path_img, 'r') as f:\n",
    "                        lines = f.readlines()\n",
    "                        f.close()\n",
    "\n",
    "                if peak_name == norm_peak:\n",
    "                    count_line = 0\n",
    "                    for line in lines:\n",
    "                        splitted_line = line.split(\" \")\n",
    "                        if splitted_line[0] != \"#\":\n",
    "                            header = count_line+1\n",
    "                            break\n",
    "                        count_line+=1\n",
    "\n",
    "                if peak_name == norm_peak:\n",
    "                    max_pixel = 0\n",
    "                    max_I = 0\n",
    "                    for line in lines[header:]:\n",
    "                        splitted_line = line.split(\" \")\n",
    "                        pixel = float(splitted_line[0])\n",
    "                        if pixel == 0:\n",
    "                            max_pixel += 1\n",
    "                        I = float(splitted_line[2])\n",
    "                        if I > max_I:\n",
    "                            max_I = I\n",
    "\n",
    "                if peak_name == norm_peak:\n",
    "                    Norm_to_fill[\"I\"] = [[] for x in range(max_pixel)]\n",
    "                    for line in lines[header:]:\n",
    "                        splitted_line = line.split(\" \")\n",
    "                        X = int(splitted_line[0]) \n",
    "                        Y = int(splitted_line[1]) \n",
    "                        I = float(splitted_line[2])\n",
    "                        In = I/max_I\n",
    "                        if peak_name == norm_peak:\n",
    "                            Norm_to_fill[\"I\"][Y].append(I)\n",
    "                    if peak_name == norm_peak:\n",
    "                        yi = 0\n",
    "                        for y in Norm_to_fill[\"I\"]:\n",
    "                            xi = 0\n",
    "                            for x in y:\n",
    "                                if Norm_to_fill[\"I\"][yi][xi] == 0:\n",
    "                                    Norm_to_fill[\"I\"][yi][xi] = 0.5\n",
    "                                xi+=1\n",
    "                            yi+=1\n",
    "                        Norm_to_fill[\"I\"] = np.array(Norm_to_fill[\"I\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b350a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sums and normalization(s)\n",
    "for directory in Samples_dir[1:]:\n",
    "    if \"Box_plots_results_img\" not in directory:\n",
    "        sample_name = directory.split(\"\\\\\")[-1]\n",
    "        #dict_all_norm_t[sample_name] = {}\n",
    "        measurements = os.listdir(directory)\n",
    "        for img_n in measurements:\n",
    "            measure_name = img_n.split(\" (\")[0]\n",
    "            peak_name = img_n.split(\") \")[1].split(\"- \")[1].split(\".txt\")[0]\n",
    "            #for peak in dict_all[sample_name][measure_name]:\n",
    "            if peak_name != \"total\":\n",
    "                dict_sums[peak_name][\"Sample\"].append(sample_name)\n",
    "                dict_sums[peak_name][\"Intensity\"].append(np.sum(dict_all[sample_name][measure_name][peak_name][\"I\"]))\n",
    "                dict_all[sample_name][measure_name][peak_name][\"I_norm_t\"] = dict_all[sample_name][measure_name][peak_name][\"I\"]/dict_all[sample_name][measure_name][\"total\"][\"I\"]\n",
    "                dict_all[sample_name][measure_name][peak_name][\"I_norm_t\"] = dict_all[sample_name][measure_name][peak_name][\"I_norm_t\"] / (max_pixel*max_pixel)\n",
    "                if len(norm_peak)>0:\n",
    "                    dict_all[sample_name][measure_name][peak_name][\"Cust_norm\"] = dict_all[sample_name][measure_name][peak_name][\"I\"]/normalization_dict[sample_name][measure_name][norm_peak][\"I\"]\n",
    "                \n",
    "                if peak_name == norm_peak and len(norm_peak)>0:\n",
    "                    yi = 0\n",
    "                    for y in dict_all[sample_name][measure_name][norm_peak][\"Cust_norm\"]:\n",
    "                        xi = 0\n",
    "                        for x in y:\n",
    "                            if dict_all[sample_name][measure_name][norm_peak][\"Cust_norm\"][yi][xi] == 0:\n",
    "                                dict_all[sample_name][measure_name][norm_peak][\"Cust_norm\"][yi][xi] = 1\n",
    "                            xi+=1\n",
    "                        yi+=1\n",
    "                        \n",
    "                dict_sums[peak_name][\"Normalized intensity\"].append(np.sum(dict_all[sample_name][measure_name][peak_name][\"I_norm_t\"]))\n",
    "                if len(norm_peak)>0:\n",
    "                    dict_sums[peak_name][\"Custom normalized intensity\"].append(np.sum(dict_all[sample_name][measure_name][peak_name][\"Cust_norm\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec8faf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating results' folder \n",
    "result_folder_name = \"Box_plots_results_img\"\n",
    "directory_res = path_data + '\\\\' + result_folder_name\n",
    "directory_res_tot = directory_res + \"\\\\\" + \"Normalization by total counts\"\n",
    "directory_res_cust = directory_res + \"\\\\\" + \"Customized normalization_\" + str(norm_peak)\n",
    "directory_res_tot_extra = directory_res_tot + '\\\\' + \"Extra\"\n",
    "directory_res_cust_extra = directory_res_cust + '\\\\' + \"Extra\"\n",
    "\n",
    "try: \n",
    "    os.makedirs(directory_res)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try: \n",
    "    os.makedirs(directory_res_tot)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if len(norm_peak)>0:\n",
    "    try: \n",
    "        os.makedirs(directory_res_cust)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "try: \n",
    "    os.makedirs(directory_res_tot_extra)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if len(norm_peak)>0:\n",
    "    try: \n",
    "        os.makedirs(directory_res_cust_extra)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c72043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the plottings\n",
    "data_per_sample_N = {}\n",
    "data_per_sample_CN = {}\n",
    "averages_N = {}\n",
    "#averages_N[\"Sample\"] = []\n",
    "averages_CN = {}\n",
    "#averages_CN[\"Sample\"] = []\n",
    "reordering = 0\n",
    "#ordered_sample_list = [\"MK-220405-BASF-25\", \"MK-220328-BASF-100\", \"MK-220325-BASF-200\", \"MK-220323-BASF-300\",\n",
    "#                      \"MK-220324-BASF-400\", \"MK-220412-BASF-500\", \"MK-220328-BASF-600\", \"MK-220324-BASF-700\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9477ac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting for original and total ions normalization\n",
    "for peak in dict_sums:\n",
    "    df_to_plot_0 = pd.DataFrame.from_dict(dict_sums[peak])\n",
    "    if reordering == 1:\n",
    "        dict_r = {}\n",
    "        dict_r[\"Sample\"] = []\n",
    "        dict_r[\"Intensity\"] = []\n",
    "        dict_r[\"Normalized intensity\"] = []\n",
    "        dict_r[\"Custom normalized intensity\"] = []\n",
    "        for s in ordered_sample_list:\n",
    "            for r in range(df_to_plot_0.shape[0]):\n",
    "                row = df_to_plot_0.iloc[r]\n",
    "                if row.iloc[0] == s:\n",
    "                    dict_r[\"Sample\"].append(row.iloc[0])\n",
    "                    dict_r[\"Intensity\"].append(row.iloc[1])\n",
    "                    dict_r[\"Normalized intensity\"].append(row.iloc[2])\n",
    "                    if len(norm_peak)>0:\n",
    "                        dict_r[\"Custom normalized intensity\"].append(row.iloc[3])\n",
    "        \n",
    "        df_to_plot = pd.DataFrame.from_dict(dict_r)\n",
    "    else:\n",
    "        df_to_plot = df_to_plot_0\n",
    "    \n",
    "    samples = []\n",
    "    for r in range(df_to_plot.shape[0]):\n",
    "        row = df_to_plot.iloc[r]\n",
    "        sample_name = row.iloc[0]\n",
    "        if row.iloc[0] not in samples:\n",
    "            samples.append(sample_name)\n",
    "            data_per_sample_N[sample_name] = []\n",
    "            data_per_sample_CN[sample_name] = []\n",
    "\n",
    "        data_per_sample_N[sample_name].append(row.iloc[2])\n",
    "        if len(norm_peak)>0:\n",
    "            data_per_sample_CN[sample_name].append(row.iloc[3])\n",
    "\n",
    "    averages_N[peak] = []\n",
    "    averages_CN[peak] = []\n",
    "    for sample_name in data_per_sample_N:\n",
    "        #averages_N[sample_name] = []\n",
    "        ave = np.average(data_per_sample_N[sample_name])\n",
    "        #averages_N[sample_name].append(ave) # Qui devo stare attento poi a legare bene il valore ave al picco \n",
    "        averages_N[peak].append(ave)\n",
    "        #averages_CN[sample_name] = []\n",
    "        ave_cn = np.average(data_per_sample_CN[sample_name])\n",
    "        #averages_CN[sample_name].append(ave_cn) # Qui devo stare attento poi a legare bene il valore ave al picco\n",
    "        averages_CN[peak].append(ave_cn)\n",
    "    \n",
    "    \n",
    "    fig = px.box(df_to_plot, x=\"Sample\", y=\"Normalized intensity\", points=\"all\", title=peak)\n",
    "    fig.update_xaxes(tickangle=45)\n",
    "    \n",
    "    if peak in important_peaks:\n",
    "        path_save = directory_res_tot\n",
    "    else :\n",
    "        path_save = directory_res_tot_extra\n",
    "    \n",
    "    csv_save = path_save + \"\\\\\" + peak + \".csv\"\n",
    "    df_to_plot.to_csv(csv_save, index=False)\n",
    "    \n",
    "    plot_save = path_save + \"\\\\\" + peak + \".png\"\n",
    "    fig.write_image(plot_save)\n",
    "    \n",
    "    #fig.show()\n",
    "    \n",
    "    #df_to_plot[\"Normalized intensity\"] = df_to_plot[\"Normalized intensity\"]/max(df_to_plot[\"Normalized intensity\"])\n",
    "    #fig = px.box(df_to_plot, x=\"Sample\", y=\"Normalized intensity\", points=\"all\", title=peak)\n",
    "    #fig.update_layout(yaxis_range=[0,1.05])\n",
    "    #fig.update_xaxes(tickangle=45)\n",
    "    \n",
    "    #if peak in important_peaks:\n",
    "    #    path_save = directory_res_tot\n",
    "    #else :\n",
    "    #    path_save = directory_res_tot_extra\n",
    "    \n",
    "    #csv_save = path_save + \"\\\\Internally_normalized_\" + peak + \".csv\"\n",
    "    #df_to_plot.to_csv(csv_save, index=False)\n",
    "    \n",
    "    #plot_save = path_save + \"\\\\Internally_normalized_\" + peak + \".png\"\n",
    "    #fig.write_image(plot_save)\n",
    "    \n",
    "    #fig.show()\n",
    "    \n",
    "ave_N_df = pd.DataFrame.from_dict(averages_N)\n",
    "ave_N_df.index = samples\n",
    "#averages_N[\"Sample\"] = samples\n",
    "csv_save = directory_res + \"\\\\All_averages_normalized_total.csv\"\n",
    "ave_N_df.to_csv(csv_save, index=True)\n",
    "\n",
    "if len(norm_peak)>0:\n",
    "    ave_CN_df = pd.DataFrame.from_dict(averages_CN)\n",
    "    ave_CN_df.index = samples\n",
    "    #averages_CN[\"Sample\"] = samples\n",
    "    csv_save = directory_res + \"\\\\All_averages_normalized_\" + str(norm_peak) + \".csv\"\n",
    "    ave_CN_df.to_csv(csv_save, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f82b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting for customized ions normalization\n",
    "if len(norm_peak)>0:\n",
    "    for peak in dict_sums:\n",
    "        df_to_plot_0 = pd.DataFrame.from_dict(dict_sums[peak])\n",
    "        if reordering == 1:\n",
    "            dict_r = {}\n",
    "            dict_r[\"Sample\"] = []\n",
    "            dict_r[\"Intensity\"] = []\n",
    "            dict_r[\"Normalized intensity\"] = []\n",
    "            dict_r[\"Custom normalized intensity\"] = []\n",
    "            for s in ordered_sample_list:\n",
    "                for r in range(df_to_plot_0.shape[0]):\n",
    "                    row = df_to_plot_0.iloc[r]\n",
    "                    if row.iloc[0] == s:\n",
    "                        dict_r[\"Sample\"].append(row.iloc[0])\n",
    "                        dict_r[\"Intensity\"].append(row.iloc[1])\n",
    "                        dict_r[\"Normalized intensity\"].append(row.iloc[2])\n",
    "                        dict_r[\"Custom normalized intensity\"].append(row.iloc[3])\n",
    "\n",
    "            df_to_plot = pd.DataFrame.from_dict(dict_r)\n",
    "        else:\n",
    "            df_to_plot = df_to_plot_0\n",
    "        \n",
    "        fig = px.box(df_to_plot, x=\"Sample\", y=\"Custom normalized intensity\", points=\"all\", title=peak)\n",
    "        fig.update_xaxes(tickangle=45)\n",
    "\n",
    "        if peak in important_peaks:\n",
    "            path_save = directory_res_cust\n",
    "        else :\n",
    "            path_save = directory_res_cust_extra\n",
    "\n",
    "        csv_save = path_save + \"\\\\\" + peak + \".csv\"\n",
    "        df_to_plot.to_csv(csv_save, index=False)\n",
    "\n",
    "        plot_save = path_save + \"\\\\\" + peak + \".png\"\n",
    "        fig.write_image(plot_save)\n",
    "\n",
    "        #fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c6cf507",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_filename = path_data + \"\\\\\" + \"dict_sums_images.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(dict_sums, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3ed027",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_filename = path_data + \"\\\\\" + \"dict_all_images.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(dict_all, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb657356",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
